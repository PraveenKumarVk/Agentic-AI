{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82c68c3-688d-4726-af7b-7572a3e90c90",
   "metadata": {},
   "source": [
    "# We are gonna follow 5 steps to create this RAG System\n",
    "1) Data Ingestion\n",
    "2) Indexing and Storing\n",
    "3) Retrieval\n",
    "4) Response Synthesis\n",
    "5) Query/Chat Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9fda0-9967-445c-bb20-b806ec277108",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5275c286-c70b-41a6-b5aa-5fdd92ac14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9751aa80-f855-43cd-b93c-108ff41dfea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "with open('chatgpt-api-credentials.yml') as file:\n",
    "    openai_key = yaml.safe_load(file)\n",
    "os.environ['OPENAI_API_KEY'] = openai_key['OPENAI-API-KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2542213f-0d68-4821-ae04-324fa9440647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['./transformers.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc89f966-3792-479c-a331-b6f34c7f410c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46748a3e-bea8-4bec-9e07-ad211cea2c0e",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1705d0d4-7d24-4f42-8486-26b96b074c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embedding_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6b6b8-f4be-4754-975b-2e0febf8185e",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d20dbe-4a37-4270-82e9-1736c6e05d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd0e05-27ab-42d4-914a-ddd91dca0193",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9418cb20-acab-4b52-97e1-59f807277721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents,embed_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306080b5-1f40-47f5-8bf8-642e3dffa9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3de099-b363-45ad-98ee-299a3a334429",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"What is self attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa0c7d30-78e5-4799-bda9-e91b4e519492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91046d92-6c43-487f-9180-3ac342fd0f38",
   "metadata": {},
   "source": [
    "# Response Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aa657ed-d73e-417d-a461-6350d6f60999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7478e-d5d2-4036-8b4c-e04a421e20a3",
   "metadata": {},
   "source": [
    "# Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f11082-59ea-4acf-93e4-213a58095217",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm = llm, response_synthesizer= response_synthesizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16b64b55-b55b-4cd2-af9f-337745cc480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"what is self attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5c7fe4c-a6b6-483e-b6d4-d362219de0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Self-attention, also known as scaled dot-product attention, is a mechanism in which the input consists of queries and keys of a certain dimension, and values of another dimension. The dot products of the query with all keys are computed, divided by the square root of the dimension of the keys, and a softmax function is applied to obtain the weights on the values. This attention function can be computed on a set of queries simultaneously, with the keys and values also packed together into matrices. This mechanism is used to allow the model to focus on different parts of the input sequence when producing an output.', source_nodes=[NodeWithScore(node=TextNode(id_='4724d83f-6043-4cb7-ad31-137841e47252', embedding=None, metadata={'page_label': '4', 'file_name': 'transformers.pdf', 'file_path': 'transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-10', 'last_modified_date': '2024-03-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d449f971-a684-44e8-84b2-bcf7511c6895', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'transformers.pdf', 'file_path': 'transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-10', 'last_modified_date': '2024-03-27'}, hash='8e64340d7bae26461355ff8ff3e07e02cb8147cef2855e7d03bba53c5dbbefac')}, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', mimetype='text/plain', start_char_idx=0, end_char_idx=2481, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4197244588347342), NodeWithScore(node=TextNode(id_='ea3d4b1f-49ab-4a7d-8e97-bba6a5b31310', embedding=None, metadata={'page_label': '13', 'file_name': 'transformers.pdf', 'file_path': 'transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-10', 'last_modified_date': '2024-03-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a5572966-8e9f-4947-bd6e-82b9a55506eb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'transformers.pdf', 'file_path': 'transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-10', 'last_modified_date': '2024-03-27'}, hash='084fe1f238d4c1b85ed6e1b21242a34ee54da260c9472effe7721d24eeb5f15e')}, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', mimetype='text/plain', start_char_idx=0, end_char_idx=812, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3680068240193059)], metadata={'4724d83f-6043-4cb7-ad31-137841e47252': {'page_label': '4', 'file_name': 'transformers.pdf', 'file_path': 'transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-10', 'last_modified_date': '2024-03-27'}, 'ea3d4b1f-49ab-4a7d-8e97-bba6a5b31310': {'page_label': '13', 'file_name': 'transformers.pdf', 'file_path': 'transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-10', 'last_modified_date': '2024-03-27'}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14538bb9-b7af-44d0-9588-6206b0d71e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'get_formatted_sources',\n",
       " 'metadata',\n",
       " 'response',\n",
       " 'source_nodes']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "552f455b-9115-427b-8880-607605cc2191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-attention, also known as scaled dot-product attention, is a mechanism in which the input consists of queries and keys of a certain dimension, and values of another dimension. The dot products of the query with all keys are computed, divided by the square root of the dimension of the keys, and a softmax function is applied to obtain the weights on the values. This attention function can be computed on a set of queries simultaneously, with the keys and values also packed together into matrices. This mechanism is used to allow the model to focus on different parts of the input sequence when producing an output.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a81ad-d00c-4185-b568-324c09624918",
   "metadata": {},
   "source": [
    "# Simplified version of all the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66359334-a655-4e99-91ce-894e69fa52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['./Transformers.pdf']).load_data()\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "embed_model = OpenAIEmbedding(model = \"text-embedding-3-large\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents,embed_model = embed_model)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
